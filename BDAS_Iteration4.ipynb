{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Opinion: integer (nullable = true)\n",
      " |-- Question: integer (nullable = true)\n",
      " |-- Answer: integer (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      " |-- Confusion: double (nullable = true)\n",
      " |-- Urgency: double (nullable = true)\n",
      " |-- CourseType: string (nullable = true)\n",
      " |-- forum_post_id: string (nullable = true)\n",
      " |-- course_display_name: string (nullable = true)\n",
      " |-- forum_uid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- anonymous: boolean (nullable = true)\n",
      " |-- anonymous_to_peers: boolean (nullable = true)\n",
      " |-- up_count: integer (nullable = true)\n",
      " |-- comment_thread_id: string (nullable = true)\n",
      " |-- reads: integer (nullable = true)\n",
      "\n",
      "2999 17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "spark = SparkSession.builder.appName('BDAS_Iteration4').getOrCreate()\n",
    "df=spark.read.load(\"./stanfordMOOCForumPostsSet.csv\", inferSchema = True,format=\"csv\",header=\"true\")\n",
    "df.printSchema()#schema details\n",
    "print(df.count(),len(df.columns))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|reads|\n",
      "+-----+\n",
      "|41   |\n",
      "|55   |\n",
      "|25   |\n",
      "|0    |\n",
      "|3    |\n",
      "|3    |\n",
      "|362  |\n",
      "|11   |\n",
      "|8    |\n",
      "|20   |\n",
      "|118  |\n",
      "|39   |\n",
      "|3    |\n",
      "|27   |\n",
      "|65   |\n",
      "|6    |\n",
      "|48   |\n",
      "|6    |\n",
      "|118  |\n",
      "|7    |\n",
      "|2    |\n",
      "|7    |\n",
      "|20   |\n",
      "|5    |\n",
      "|2    |\n",
      "|17   |\n",
      "|3    |\n",
      "|4    |\n",
      "|150  |\n",
      "|10   |\n",
      "|1    |\n",
      "|5    |\n",
      "|1117 |\n",
      "|17   |\n",
      "|2    |\n",
      "|3    |\n",
      "|12   |\n",
      "|47   |\n",
      "|3    |\n",
      "|19   |\n",
      "+-----+\n",
      "only showing top 40 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('Data')\n",
    "Results = spark.sql(\"SELECT reads FROM Data\")\n",
    "Results.show(40,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|up_count|\n",
      "+--------+\n",
      "|28      |\n",
      "|12      |\n",
      "|1       |\n",
      "|13      |\n",
      "|6       |\n",
      "|16      |\n",
      "|3       |\n",
      "|20      |\n",
      "|5       |\n",
      "|191     |\n",
      "|15      |\n",
      "|37      |\n",
      "|9       |\n",
      "|4       |\n",
      "|8       |\n",
      "|7       |\n",
      "|10      |\n",
      "|73      |\n",
      "|25      |\n",
      "|24      |\n",
      "|75      |\n",
      "|11      |\n",
      "|14      |\n",
      "|2       |\n",
      "|0       |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('Data')\n",
    "Results = spark.sql(\"SELECT DISTINCT up_count FROM Data\")\n",
    "Results.show(40,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Confusion|\n",
      "+---------+\n",
      "|7.0      |\n",
      "|3.5      |\n",
      "|4.5      |\n",
      "|6.5      |\n",
      "|2.5      |\n",
      "|1.0      |\n",
      "|4.0      |\n",
      "|3.0      |\n",
      "|2.0      |\n",
      "|1.5      |\n",
      "|6.0      |\n",
      "|5.0      |\n",
      "|5.5      |\n",
      "+---------+\n",
      "\n",
      "['Opinion', 'Question', 'Answer', 'Sentiment', 'Confusion', 'Urgency', 'CourseType', 'forum_post_id', 'course_display_name', 'forum_uid', 'created_at', 'post_type', 'anonymous', 'anonymous_to_peers', 'up_count', 'comment_thread_id', 'reads']\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('Data')\n",
    "Results = spark.sql(\"SELECT DISTINCT Confusion FROM Data\")\n",
    "Results.show(40,False)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Opinion', 'Question', 'Answer', 'Sentiment', 'Confusion', 'Urgency', 'CourseType', 'forum_post_id', 'course_display_name', 'forum_uid', 'created_at', 'post_type', 'up_count', 'comment_thread_id', 'reads']\n"
     ]
    }
   ],
   "source": [
    "newdf=df;\n",
    "df=df.drop('anonymous','anonymous_to_peers')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|summary|Opinion|Question|Answer|Sentiment|Confusion|Urgency|CourseType|forum_post_id|course_display_name|forum_uid|created_at|post_type|up_count|comment_thread_id|reads|\n",
      "+-------+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|  count|   2999|    2999|  2999|     2999|     2999|   2999|      2999|         2999|               2999|     2999|      2999|     2999|    2999|             2999| 2999|\n",
      "+-------+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|Opinion|Question|Answer|Sentiment|Confusion|Urgency|CourseType|forum_post_id|course_display_name|forum_uid|created_at|post_type|up_count|comment_thread_id|reads|\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|      0|       0|     0|        0|        0|      0|         0|            0|                  0|        0|         0|        0|       0|                0|    0|\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df.describe().filter(col(\"summary\") == \"count\").show()\n",
    "df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+------------------+-----------------+------------------+----------+--------------------+--------------------+--------------------+----------------+-------------+------------------+--------------------+------------------+\n",
      "|summary|            Opinion|           Question|             Answer|         Sentiment|        Confusion|           Urgency|CourseType|       forum_post_id| course_display_name|           forum_uid|      created_at|    post_type|          up_count|   comment_thread_id|             reads|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+-----------------+------------------+----------+--------------------+--------------------+--------------------+----------------+-------------+------------------+--------------------+------------------+\n",
      "|  count|               2999|               2999|               2999|              2999|             2999|              2999|      2999|                2999|                2999|                2999|            2999|         2999|              2999|                2999|              2999|\n",
      "|   mean|  0.464488162720907|0.20406802267422475| 0.1983994664888296|  4.42597532510837|3.837945981993998|2.9579859953317773|      null|            Infinity|                null|                null|            null|         null|0.5608536178726242|                null| 594.5225075025008|\n",
      "| stddev|0.49882048644282384|  0.403086147536746|0.39886108599820125|0.8694976661952746|0.656151104495741| 1.357803802437799|      null|                 NaN|                null|                null|            null|         null| 4.293852914747049|                null|1547.2966934135623|\n",
      "|    min|                  0|                  0|                  0|               1.0|              1.0|               1.0| Education|51b513008e8d330d0...|Education/EDUC115...|00206D49076D88F28...|01-08-2013 01:08|      Comment|                 0|51b513008e8d330d0...|                 0|\n",
      "|    max|                  1|                  1|                  1|               7.0|              7.0|               7.0|  Medicine|52dea57953cc4488d...|Medicine/HRP258/S...|FFEB1A5597A01AD27...|31-08-2013 22:10|CommentThread|               191|                None|              9452|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+-----------------+------------------+----------+--------------------+--------------------+--------------------+----------------+-------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|reads|\n",
      "+-----+\n",
      "|   41|\n",
      "|   55|\n",
      "|   25|\n",
      "|    0|\n",
      "|    3|\n",
      "|    3|\n",
      "|  362|\n",
      "|   11|\n",
      "|    8|\n",
      "|   20|\n",
      "|  118|\n",
      "|   39|\n",
      "|    3|\n",
      "|   27|\n",
      "|   65|\n",
      "|    6|\n",
      "|   48|\n",
      "|    6|\n",
      "|  118|\n",
      "|    7|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('reads').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Opinion', 'Question', 'Answer', 'Sentiment', 'Confusion', 'Urgency', 'CourseType', 'course_display_name', 'created_at', 'post_type', 'anonymous', 'anonymous_to_peers', 'up_count', 'comment_thread_id', 'reads']\n"
     ]
    }
   ],
   "source": [
    "newdf=newdf.drop('forum_post_id','forum_uid')\n",
    "print(newdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Opinion: integer (nullable = true)\n",
      " |-- Question: integer (nullable = true)\n",
      " |-- Answer: integer (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      " |-- Confusion: double (nullable = true)\n",
      " |-- Urgency: double (nullable = true)\n",
      " |-- CourseType: string (nullable = true)\n",
      " |-- course_display_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- anonymous: boolean (nullable = true)\n",
      " |-- anonymous_to_peers: boolean (nullable = true)\n",
      " |-- up_count: integer (nullable = true)\n",
      " |-- comment_thread_id: string (nullable = true)\n",
      " |-- reads: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newdf.printSchema()#schema details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Opinion: integer (nullable = true)\n",
      " |-- Question: integer (nullable = true)\n",
      " |-- Answer: integer (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      " |-- Confusion: double (nullable = true)\n",
      " |-- Urgency: double (nullable = true)\n",
      " |-- CourseType: string (nullable = true)\n",
      " |-- forum_post_id: string (nullable = true)\n",
      " |-- course_display_name: string (nullable = true)\n",
      " |-- forum_uid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- up_count: integer (nullable = true)\n",
      " |-- comment_thread_id: string (nullable = true)\n",
      " |-- reads: integer (nullable = true)\n",
      "\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|Opinion|Question|Answer|Sentiment|Confusion|Urgency|CourseType|forum_post_id|course_display_name|forum_uid|created_at|post_type|up_count|comment_thread_id|reads|\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "|      0|       0|     0|        0|        0|      0|         0|            0|                  0|        0|         0|        0|       0|                0|    0|\n",
      "+-------+--------+------+---------+---------+-------+----------+-------------+-------------------+---------+----------+---------+--------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dropping any Null and missing values\n",
    "df.printSchema()\n",
    "df.dropna()\n",
    "df.select([count(when(isnan(c) | col(c).isNull(),c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opinion \t with null values:  0\n",
      "Question \t with null values:  0\n",
      "Answer \t with null values:  0\n",
      "Sentiment \t with null values:  0\n",
      "Confusion \t with null values:  0\n",
      "Urgency \t with null values:  0\n",
      "CourseType \t with null values:  0\n",
      "course_display_name \t with null values:  0\n",
      "created_at \t with null values:  0\n",
      "post_type \t with null values:  0\n",
      "anonymous \t with null values:  0\n",
      "anonymous_to_peers \t with null values:  0\n",
      "up_count \t with null values:  0\n",
      "comment_thread_id \t with null values:  0\n",
      "reads \t with null values:  0\n"
     ]
    }
   ],
   "source": [
    "for col in newdf.columns:\n",
    "    print(col,\"\\t\",\"with null values: \",newdf.filter(newdf[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Urgency|\n",
      "+-------+\n",
      "|7.0    |\n",
      "|3.5    |\n",
      "|4.5    |\n",
      "|6.5    |\n",
      "|2.5    |\n",
      "|4.0    |\n",
      "|3.0    |\n",
      "|2.0    |\n",
      "|1.5    |\n",
      "|6.0    |\n",
      "|5.0    |\n",
      "|5.5    |\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning for Urgency\n",
    "import pyspark.sql.functions as fun\n",
    "newdf = newdf.withColumn(\n",
    "\"Urgency\",\n",
    "    fun.when(\n",
    "        (fun.col(\"Urgency\") >1)&\n",
    "        (fun.col(\"Urgency\") <=7), fun.col(\"Urgency\")).otherwise(fun.lit(None))\n",
    ")\n",
    "na_df=newdf.dropna()\n",
    "na_df.select('Urgency').distinct().show(1000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Sentiment|\n",
      "+---------+\n",
      "|7.0      |\n",
      "|3.5      |\n",
      "|4.5      |\n",
      "|6.5      |\n",
      "|2.5      |\n",
      "|4.0      |\n",
      "|3.0      |\n",
      "|2.0      |\n",
      "|1.5      |\n",
      "|6.0      |\n",
      "|5.0      |\n",
      "|5.5      |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data Cleaning for Sentiment\n",
    "import pyspark.sql.functions as fun\n",
    "newdf = newdf.withColumn(\n",
    "\"Sentiment\",\n",
    "    fun.when(\n",
    "        (fun.col(\"Sentiment\") >1)&\n",
    "        (fun.col(\"Sentiment\") <=7), fun.col(\"Sentiment\")).otherwise(fun.lit(None))\n",
    ")\n",
    "na_df=newdf.dropna()\n",
    "na_df.select('Sentiment').distinct().show(1000,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|min(Urgency)|max(Urgency)|\n",
      "+------------+------------+\n",
      "|1.5         |7.0         |\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_df.createOrReplaceTempView('data')\n",
    "Results = spark.sql(\"SELECT min(Urgency), max(Urgency) FROM data\")\n",
    "Results.show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(Confusion)|max(Confusion)|\n",
      "+--------------+--------------+\n",
      "|1.0           |6.5           |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_df.createOrReplaceTempView('data')\n",
    "Results = spark.sql(\"SELECT min(Confusion), max(Confusion) FROM data\")\n",
    "Results.show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|min(Sentiment)|max(Sentiment)|\n",
      "+--------------+--------------+\n",
      "|1.5           |7.0           |\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "na_df.createOrReplaceTempView('data')\n",
    "Results = spark.sql(\"SELECT min(Sentiment), max(Sentiment) FROM data\")\n",
    "Results.show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Participation|\n",
      "+-------------+\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "|         true|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Opinion: integer (nullable = true)\n",
      " |-- Question: integer (nullable = true)\n",
      " |-- Answer: integer (nullable = true)\n",
      " |-- Sentiment: double (nullable = true)\n",
      " |-- Confusion: double (nullable = true)\n",
      " |-- Urgency: double (nullable = true)\n",
      " |-- CourseType: string (nullable = true)\n",
      " |-- course_display_name: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- anonymous: boolean (nullable = true)\n",
      " |-- anonymous_to_peers: boolean (nullable = true)\n",
      " |-- up_count: integer (nullable = true)\n",
      " |-- comment_thread_id: string (nullable = true)\n",
      " |-- reads: integer (nullable = true)\n",
      " |-- Participation: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deriving New Column\n",
    "df1 = na_df.withColumn('Participation',(fun.col('Opinion') == 1 ) | (fun.col('Question') == 1) | (fun.col('Answer') == 1))\n",
    "df1.select('Participation').show()\n",
    "df1.printSchema()#schema details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878 16\n",
      "993 16\n",
      "3871 16\n"
     ]
    }
   ],
   "source": [
    "#Merging the data\n",
    "data2 = spark.read.load(\"./stanfordMOOCForumPostsSet2.csv\",format=\"csv\", inferSchema = True, header=\"true\")\n",
    "newdf2 = data2.drop('forum_post_id','forum_uid')\n",
    "import pyspark.sql.functions as fun\n",
    "newdf2 = newdf2.withColumn(\n",
    "\"Urgency\",\n",
    "    fun.when(\n",
    "        (fun.col(\"Urgency\") >1)&\n",
    "        (fun.col(\"Urgency\") <=7), fun.col(\"Urgency\")).otherwise(fun.lit(None))\n",
    ")\n",
    "newdf2 = newdf2.withColumn(\n",
    "\"Sentiment\",\n",
    "    fun.when(\n",
    "        (fun.col(\"Sentiment\") >1)&\n",
    "        (fun.col(\"Sentiment\") <=7), fun.col(\"Sentiment\")).otherwise(fun.lit(None))\n",
    ")\n",
    "newdf2 = newdf2.withColumn(\n",
    "\"Confusion\",\n",
    "    fun.when(\n",
    "        (fun.col(\"Confusion\") >1)&\n",
    "        (fun.col(\"Confusion\") <=7), fun.col(\"Confusion\")).otherwise(fun.lit(None))\n",
    ")\n",
    "na_df2 = newdf2.dropna()\n",
    "#print(na_df2.count(),len(na_df2.columns))\n",
    "df2 = na_df2.withColumn('Participation',(fun.col('Opinion') == 1 ) | (fun.col('Question') == 1) | (fun.col('Answer') == 1))\n",
    "print(df1.count(),len(df1.columns))\n",
    "print(df2.count(),len(df2.columns))\n",
    "#Merging\n",
    "finaldf = df1.union(df2)\n",
    "print(finaldf.count(),len(finaldf.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting string value to integer \n",
    "from pyspark.ml.feature import StringIndexer\n",
    "encodeddf = finaldf\n",
    "x = finaldf.columns\n",
    "x.remove(\"anonymous\") #Its already boolean\n",
    "x.remove(\"anonymous_to_peers\") #Its already boolean\n",
    "x.remove(\"Participation\") #Its already boolean\n",
    "for i in x:\n",
    "    ic = i\n",
    "    oc = i+\" encoded\" \n",
    "    indexer = StringIndexer(inputCol=ic, outputCol=oc)\n",
    "    encodeddf = indexer.fit(encodeddf).transform(encodeddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i for i in encodeddf.columns if i not in x] #storing only those columns which are numerical\n",
    "encodeddf = encodeddf[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+-------------+---------------+----------------+--------------+-----------------+-----------------+---------------+------------------+---------------------------+------------------+-----------------+----------------+-------------------------+-------------+\n",
      "|anonymous|anonymous_to_peers|Participation|Opinion encoded|Question encoded|Answer encoded|Sentiment encoded|Confusion encoded|Urgency encoded|CourseType encoded|course_display_name encoded|created_at encoded|post_type encoded|up_count encoded|comment_thread_id encoded|reads encoded|\n",
      "+---------+------------------+-------------+---------------+----------------+--------------+-----------------+-----------------+---------------+------------------+---------------------------+------------------+-----------------+----------------+-------------------------+-------------+\n",
      "|    false|             false|         true|            0.0|             0.0|           0.0|              7.0|              7.0|            1.0|               0.0|                        0.0|            3272.0|              0.0|             0.0|                   1178.0|         36.0|\n",
      "|    false|             false|         true|            1.0|             1.0|           0.0|              0.0|              4.0|            4.0|               0.0|                        0.0|            3501.0|              0.0|             0.0|                    469.0|         99.0|\n",
      "|    false|             false|         true|            0.0|             0.0|           0.0|              4.0|              2.0|            3.0|               0.0|                        0.0|            1695.0|              0.0|             0.0|                   1208.0|         48.0|\n",
      "|    false|             false|         true|            0.0|             0.0|           0.0|              5.0|              2.0|            3.0|               0.0|                        0.0|            1167.0|              1.0|             0.0|                      0.0|         11.0|\n",
      "|    false|             false|         true|            0.0|             0.0|           0.0|              9.0|              7.0|            2.0|               0.0|                        0.0|             778.0|              1.0|             0.0|                      0.0|          2.0|\n",
      "+---------+------------------+-------------+---------------+----------------+--------------+-----------------+-----------------+---------------+------------------+---------------------------+------------------+-----------------+----------------+-------------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodeddf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anonymous: boolean (nullable = true)\n",
      " |-- anonymous_to_peers: boolean (nullable = true)\n",
      " |-- Participation: boolean (nullable = true)\n",
      " |-- Opinion encoded: double (nullable = true)\n",
      " |-- Question encoded: double (nullable = true)\n",
      " |-- Answer encoded: double (nullable = true)\n",
      " |-- Sentiment encoded: double (nullable = true)\n",
      " |-- Confusion encoded: double (nullable = true)\n",
      " |-- Urgency encoded: double (nullable = true)\n",
      " |-- CourseType encoded: double (nullable = true)\n",
      " |-- course_display_name encoded: double (nullable = true)\n",
      " |-- created_at encoded: double (nullable = true)\n",
      " |-- post_type encoded: double (nullable = true)\n",
      " |-- up_count encoded: double (nullable = true)\n",
      " |-- comment_thread_id encoded: double (nullable = true)\n",
      " |-- reads encoded: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encodeddf.printSchema()\n",
    "df=encodeddf;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "replacements = {c:c.replace(' encoded','') for c in df.columns }\n",
    "\n",
    "df = df.select([col(c).alias(replacements.get(c, c)) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2200x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#4.1 Data deduction - correlation matrix with heatmap \n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "resultRDD = df.rdd.map(lambda row: row[0:])\n",
    "corrlation_matrix = Statistics.corr(resultRDD, method = 'pearson')\n",
    "rdd0 = sc.parallelize(corrlation_matrix)\n",
    "rdd1 = rdd0.map(lambda x: [float(i) for i in x])\n",
    "corrlation_matrix[1]\n",
    "\n",
    "plt.figure(figsize=(22,10))\n",
    "sns.heatmap(corrlation_matrix, cmap=\"Purples\", annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert columns and transform them to numerical\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "column_vec_in = [\n",
    "                'Opinion',\n",
    "                'Question',\n",
    "                'Answer',\n",
    "                'Sentiment',\n",
    "                'Confusion',\n",
    "                'Urgency',\n",
    "                'course_display_name',\n",
    "                'created_at',\n",
    "                'post_type',\n",
    "                'up_count',\n",
    "                'comment_thread_id',\n",
    "                'reads'\n",
    "                ]\n",
    "\n",
    "column_vec_out = ['Opinion_Vec',\n",
    "                'Question_Vec',\n",
    "                'Answer_Vec',\n",
    "                'Sentiment_Vec',\n",
    "                'Confusion_Vec',\n",
    "                'Urgency_Vec',\n",
    "                'course_display_name_Vec',\n",
    "                'created_at_Vec',\n",
    "                'post_type_Vec',\n",
    "                'up_count_Vec',\n",
    "                'comment_thread_id_Vec',\n",
    "                'reads_Vec']\n",
    "\n",
    "indexers = [StringIndexer(inputCol = x, outputCol=x + \"_tmp\") for x in column_vec_in]\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=False, inputCol = x + \"_tmp\", outputCol = y)\n",
    "           for x, y in zip(column_vec_in, column_vec_out)]\n",
    "\n",
    "tmp = [[i, j] for i, j in zip(indexers, encoders)]\n",
    "tmp = [i for sublist in tmp for i in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining atttributes into the of features and target is Coursetype\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cols_now = ['Opinion_Vec',\n",
    "                'Question_Vec',\n",
    "                'Answer_Vec',\n",
    "                'Sentiment_Vec',\n",
    "                'Confusion_Vec',\n",
    "                'Urgency_Vec',\n",
    "                'course_display_name_Vec',\n",
    "                'created_at_Vec',\n",
    "                'post_type_Vec',\n",
    "                'up_count_Vec',\n",
    "                'comment_thread_id_Vec',\n",
    "                'reads_Vec']\n",
    "\n",
    "assembler_features = VectorAssembler(inputCols = cols_now, outputCol= \"features\")\n",
    "labelIndexer = StringIndexer(inputCol = 'CourseType', outputCol= \"label\")\n",
    "tmp += [assembler_features, labelIndexer]\n",
    "pipeline = Pipeline(stages = tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Positive and Negative in trainingData is:  [Row(label=0.0, count=1395), Row(label=1.0, count=695), Row(label=2.0, count=650)]\n"
     ]
    }
   ],
   "source": [
    "#Training Data split 70% and 30%\n",
    "allData = pipeline.fit(df).transform(df)\n",
    "allData.cache()\n",
    "\n",
    "trainingData, testData = allData.randomSplit([0.7, 0.3], seed=0)\n",
    "print(\"Distribution of Positive and Negative in trainingData is: \", trainingData.groupBy(\"label\").count().take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5313, {0: 0.0458, 1: 0.0603, 2: 0.0089, 3: 0.0162, 4: 0.0102, 5: 0.0358, 6: 0.04, 7: 0.0164, 8: 0.0009, 9: 0.0024, 10: 0.0072, 12: 0.0049, 13: 0.006, 14: 0.0, 15: 0.0011, 18: 0.0244, 19: 0.0219, 20: 0.0065, 21: 0.0063, 22: 0.0091, 23: 0.0073, 24: 0.0034, 25: 0.0001, 26: 0.0017, 30: 0.0167, 31: 0.0121, 32: 0.01, 34: 0.0002, 35: 0.012, 36: 0.0033, 37: 0.0157, 38: 0.0016, 39: 0.009, 40: 0.0031, 41: 0.0002, 42: 0.041, 43: 0.0689, 44: 0.0293, 45: 0.0132, 46: 0.02, 47: 0.001, 50: 0.0012, 60: 0.0, 68: 0.0011, 77: 0.0, 81: 0.0002, 83: 0.0003, 92: 0.0001, 114: 0.0012, 126: 0.0007, 140: 0.0017, 146: 0.0003, 147: 0.0001, 153: 0.0001, 159: 0.0, 184: 0.0, 193: 0.0001, 231: 0.0, 232: 0.0002, 302: 0.0, 355: 0.0001, 361: 0.0, 371: 0.0006, 381: 0.0006, 395: 0.0001, 420: 0.0, 428: 0.0, 457: 0.0004, 486: 0.0, 540: 0.0001, 541: 0.0, 575: 0.0006, 576: 0.0, 609: 0.0002, 626: 0.001, 677: 0.0, 690: 0.0, 728: 0.0003, 798: 0.0007, 804: 0.0002, 815: 0.0, 818: 0.0001, 828: 0.0, 845: 0.0, 891: 0.0002, 946: 0.0001, 955: 0.0, 991: 0.0001, 1037: 0.0, 1071: 0.0, 1103: 0.0, 1147: 0.0, 1173: 0.0005, 1206: 0.0, 1238: 0.0, 1302: 0.0, 1309: 0.0003, 1338: 0.0, 1379: 0.0004, 1382: 0.0002, 1430: 0.0, 1512: 0.0001, 1518: 0.0, 1645: 0.0003, 1665: 0.0001, 1667: 0.0, 1707: 0.0001, 1718: 0.0002, 1723: 0.0, 1760: 0.0003, 1842: 0.0, 1885: 0.0, 1937: 0.0002, 1943: 0.0, 1964: 0.0002, 1996: 0.0, 2148: 0.0002, 2165: 0.0, 2174: 0.0001, 2188: 0.0, 2192: 0.0, 2206: 0.0, 2216: 0.0002, 2250: 0.0, 2288: 0.0, 2295: 0.0, 2327: 0.0002, 2465: 0.0, 2467: 0.0, 2510: 0.0, 2518: 0.0, 2629: 0.0, 2635: 0.0, 2809: 0.0003, 2819: 0.0001, 2822: 0.0001, 2857: 0.0, 2886: 0.0001, 3104: 0.0001, 3127: 0.0, 3130: 0.0001, 3174: 0.0, 3175: 0.0003, 3223: 0.0003, 3302: 0.0002, 3355: 0.0, 3441: 0.0, 3553: 0.0003, 3667: 0.0002, 3728: 0.0, 3764: 0.0, 3797: 0.0, 3801: 0.0118, 3802: 0.0179, 3803: 0.0068, 3804: 0.0051, 3805: 0.0, 3808: 0.0024, 3813: 0.0005, 3814: 0.0, 3828: 0.0156, 3829: 0.0147, 3830: 0.0063, 3831: 0.0088, 3832: 0.008, 3833: 0.0004, 3835: 0.0014, 3838: 0.0025, 3839: 0.0014, 3843: 0.0018, 3844: 0.0, 3845: 0.0, 3846: 0.0004, 3849: 0.0019, 3850: 0.0001, 3852: 0.0005, 3853: 0.0004, 3855: 0.0, 3858: 0.0004, 3860: 0.0007, 3861: 0.0002, 3863: 0.0018, 3864: 0.0015, 3865: 0.0003, 3868: 0.0, 3869: 0.0003, 3877: 0.0011, 3878: 0.0014, 3880: 0.0001, 3883: 0.0, 3885: 0.0001, 3887: 0.0002, 3893: 0.0001, 3896: 0.0, 3897: 0.0002, 3899: 0.0005, 3906: 0.0018, 3913: 0.0005, 3917: 0.0003, 3919: 0.0004, 3925: 0.0009, 3929: 0.0029, 3934: 0.0, 3938: 0.0, 3966: 0.0, 3980: 0.0013, 3981: 0.0001, 3982: 0.0002, 3987: 0.0006, 3995: 0.0, 3997: 0.0, 4009: 0.0, 4024: 0.0001, 4049: 0.0001, 4052: 0.0013, 4060: 0.0002, 4061: 0.0012, 4070: 0.0004, 4097: 0.0, 4104: 0.0001, 4115: 0.0001, 4127: 0.0, 4146: 0.0, 4159: 0.0001, 4199: 0.0, 4210: 0.0, 4241: 0.0001, 4273: 0.0, 4335: 0.0, 4389: 0.0002, 4394: 0.0, 4520: 0.0, 4641: 0.0, 4677: 0.0, 4678: 0.0001, 4714: 0.0, 4764: 0.0002, 4775: 0.0012, 4880: 0.0, 4939: 0.0005, 5001: 0.0, 5026: 0.0, 5038: 0.0362, 5039: 0.0113, 5040: 0.0072, 5041: 0.0144, 5042: 0.0056, 5043: 0.0123, 5044: 0.0081, 5045: 0.0098, 5046: 0.0075, 5047: 0.0031, 5048: 0.0058, 5049: 0.0034, 5050: 0.0037, 5051: 0.0051, 5052: 0.0017, 5053: 0.0004, 5054: 0.0021, 5055: 0.0013, 5057: 0.0003, 5058: 0.0073, 5059: 0.0038, 5063: 0.0, 5064: 0.0, 5065: 0.0, 5066: 0.0001, 5073: 0.0058, 5074: 0.0003, 5075: 0.0011, 5076: 0.0074, 5077: 0.0, 5078: 0.0044, 5079: 0.0044, 5081: 0.0026, 5082: 0.0008, 5087: 0.0002, 5090: 0.003, 5092: 0.0033, 5095: 0.0001, 5096: 0.0003, 5100: 0.0038, 5101: 0.0017, 5105: 0.0008, 5108: 0.0002, 5110: 0.0032, 5112: 0.0021, 5114: 0.0007, 5115: 0.003, 5116: 0.0006, 5117: 0.0003, 5118: 0.0007, 5120: 0.0025, 5123: 0.0035, 5124: 0.0001, 5125: 0.0051, 5127: 0.0014, 5128: 0.0006, 5129: 0.0028, 5130: 0.0001, 5131: 0.0004, 5132: 0.0006, 5133: 0.0004, 5135: 0.0001, 5136: 0.0003, 5138: 0.0, 5139: 0.0017, 5140: 0.0, 5142: 0.0017, 5143: 0.0011, 5145: 0.0038, 5148: 0.0018, 5149: 0.0001, 5150: 0.0012, 5152: 0.0002, 5153: 0.0012, 5154: 0.0002, 5156: 0.0005, 5160: 0.0005, 5161: 0.0, 5162: 0.0, 5163: 0.0001, 5164: 0.0014, 5165: 0.0016, 5166: 0.0001, 5168: 0.0, 5170: 0.0001, 5171: 0.0006, 5172: 0.0003, 5173: 0.0007, 5176: 0.0021, 5178: 0.0004, 5180: 0.0025, 5181: 0.0, 5182: 0.0022, 5183: 0.0014, 5185: 0.0, 5186: 0.0006, 5188: 0.0, 5191: 0.0, 5193: 0.0, 5194: 0.0009, 5198: 0.0011, 5199: 0.0003, 5202: 0.0, 5206: 0.0001, 5208: 0.0, 5209: 0.0004, 5217: 0.0001, 5227: 0.0005, 5228: 0.0008, 5230: 0.0003, 5231: 0.0, 5248: 0.0002, 5257: 0.0005, 5258: 0.0002})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Classifer\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "rf = RF(labelCol = \"label\", featuresCol = \"features\", numTrees = 200, maxDepth = 3)\n",
    "fit = rf.fit(trainingData)\n",
    "transformed = fit.transform(testData)\n",
    "\n",
    "fit.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is:  0.9959700181898014\n",
      "Area under PR = 0.9967183647218204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.56      1.00      0.72       596\n",
      "         1.0       1.00      0.01      0.02       271\n",
      "         2.0       1.00      0.24      0.39       264\n",
      "\n",
      "    accuracy                           0.59      1131\n",
      "   macro avg       0.85      0.42      0.37      1131\n",
      "weighted avg       0.77      0.59      0.47      1131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6.3.2 Random Forest\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "results = transformed.select(['probability', 'label'])\n",
    "\n",
    "results_collect = results.collect()\n",
    "results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]\n",
    "\n",
    "scoreAndLabels = sc.parallelize(results_list)\n",
    "metrics = metric(scoreAndLabels)\n",
    "print(\"The ROC score is: \", metrics.areaUnderROC)\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_true = transformed.select(['label']).collect()\n",
    "y_pred = transformed.select(['prediction']).collect()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5313, {42: 0.6047, 43: 0.3953})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier as DT\n",
    "tree = DT(labelCol = \"label\", featuresCol = \"features\", maxDepth = 3)\n",
    "tree_fit = tree.fit(trainingData)\n",
    "tree_transformed = tree_fit.transform(testData)\n",
    "\n",
    "tree_fit.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC score is:  1.0\n",
      "Area under PR = 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       596\n",
      "         1.0       1.00      1.00      1.00       271\n",
      "         2.0       1.00      1.00      1.00       264\n",
      "\n",
      "    accuracy                           1.00      1131\n",
      "   macro avg       1.00      1.00      1.00      1131\n",
      "weighted avg       1.00      1.00      1.00      1131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "tree_results = tree_transformed.select(['probability', 'label'])\n",
    "\n",
    "tree_results_collect = tree_results.collect()\n",
    "tree_results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in tree_results_collect]\n",
    "\n",
    "tree_scoreAndLabels = sc.parallelize(tree_results_list)\n",
    "tree_metrics = metric(tree_scoreAndLabels)\n",
    "print(\"The ROC score is: \", tree_metrics.areaUnderROC)\n",
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % tree_metrics.areaUnderPR)\n",
    "\n",
    "\n",
    "y_true = tree_transformed.select(['label']).collect()\n",
    "y_pred = tree_transformed.select(['prediction']).collect()\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier\n",
      "0.7532710280373831\n",
      "Random Forest Classifer\n",
      "0.8606269209057278\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#7 Data Mining\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "dtc = DecisionTreeClassifier(labelCol='label',featuresCol='features')\n",
    "rfc = RandomForestClassifier(labelCol='label',featuresCol='features')\n",
    "\n",
    "dtc_model = dtc.fit(trainingData)\n",
    "rfc_model = rfc.fit(trainingData)\n",
    "\n",
    "\n",
    "dtc_predictions = dtc_model.transform(testData)\n",
    "rfc_predictions = rfc_model.transform(testData)\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "my_binary_eval = BinaryClassificationEvaluator(labelCol = 'label')\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(my_binary_eval.evaluate(dtc_predictions))\n",
    "\n",
    "print(\"Random Forest Classifer\")\n",
    "print(my_binary_eval.evaluate(rfc_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the results!\n",
      "----------------------------------------\n",
      "A single decision tree has an accuracy of: 100.00%\n",
      "----------------------------------------\n",
      "A random forest ensemble has an accuracy of: 63.22%\n"
     ]
    }
   ],
   "source": [
    "# Data Mining\n",
    "#Evaluating Accuracy for all test model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "\n",
    "print(\"Here are the results!\")\n",
    "print('-'*40)\n",
    "print('A single decision tree has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('-'*40)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5313, {42: 0.8177, 43: 0.1823})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Decision Tree Features Important\n",
    "from pyspark.ml.regression import DecisionTreeRegressor as DTR\n",
    "dtr_tree = DTR(labelCol = \"label\", featuresCol = \"features\", maxDepth = 3)\n",
    "dtr_tree_fit = dtr_tree.fit(trainingData)\n",
    "dtr_tree_transformed = dtr_tree_fit.transform(testData)\n",
    "\n",
    "dtr_tree_fit.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error is:  0.376657824933687\n",
      "Mean Squared Error:  0.4792219274977896\n",
      "Root Mean Squared Error is:  0.6922585698261811\n",
      "R2 is:  0.2892203015644059\n",
      "Explained Variance is:  0.34683827915328475\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics as metric\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "dtr_results = dtr_tree_transformed.select(['features', 'label'])\n",
    "\n",
    "dtr_results_collect = dtr_results.collect()\n",
    "dtr_results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in dtr_results_collect]\n",
    "\n",
    "dtr_scoreAndLabels = sc.parallelize(dtr_results_list)\n",
    "dtr_metrics = metric(dtr_scoreAndLabels)\n",
    "print(\"Mean Absolute Error is: \", dtr_metrics.meanAbsoluteError)\n",
    "print(\"Mean Squared Error: \", dtr_metrics.meanSquaredError)\n",
    "print(\"Root Mean Squared Error is: \", dtr_metrics.rootMeanSquaredError)\n",
    "print(\"R2 is: \", dtr_metrics.r2)\n",
    "print(\"Explained Variance is: \", dtr_metrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5313, {0: 0.1046, 1: 0.0516, 4: 0.0156, 5: 0.0058, 6: 0.0061, 18: 0.0003, 19: 0.0125, 25: 0.0002, 30: 0.0007, 36: 0.0003, 42: 0.549, 43: 0.1995, 44: 0.0471, 45: 0.0007, 46: 0.0009, 3828: 0.001, 3829: 0.0009, 3830: 0.0006, 5038: 0.002, 5053: 0.0005})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random Forest Feature Importances\n",
    "from pyspark.ml.regression import RandomForestRegressor as RFR\n",
    "rfr_tree = RFR(labelCol = \"label\", featuresCol = \"features\", maxDepth = 3)\n",
    "rfr_tree_fit = rfr_tree.fit(trainingData)\n",
    "rfr_tree_transformed = rfr_tree_fit.transform(testData)\n",
    "\n",
    "rfr_tree_fit.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error is:  0.376657824933687\n",
      "Mean Squared Error:  0.4792219274977896\n",
      "Root Mean Squared Error is:  0.6922585698261811\n",
      "R2 is:  0.2892203015644059\n",
      "Explained Variance is:  0.34683827915328475\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics as metric\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "rfr_results = rfr_tree_transformed.select(['features', 'label'])\n",
    "\n",
    "rfr_results_collect = rfr_results.collect()\n",
    "rfr_results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in rfr_results_collect]\n",
    "\n",
    "rfr_scoreAndLabels = sc.parallelize(rfr_results_list)\n",
    "rfr_metrics = metric(rfr_scoreAndLabels)\n",
    "print(\"Mean Absolute Error is: \", rfr_metrics.meanAbsoluteError)\n",
    "print(\"Mean Squared Error: \", rfr_metrics.meanSquaredError)\n",
    "print(\"Root Mean Squared Error is: \", rfr_metrics.rootMeanSquaredError)\n",
    "print(\"R2 is: \", rfr_metrics.r2)\n",
    "print(\"Explained Variance is: \", rfr_metrics.explainedVariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 10 features selected\n",
      "+---------+------------------+-------------+-------+--------+------+---------+---------+-------+----------+-------------------+----------+---------+--------+-----------------+-----+-----------+-------------+------------+-------------+----------+-------------+-------------+--------------+-------------+---------------+-----------+--------------+-----------------------+-----------------------+--------------+-------------------+-------------+-------------+------------+--------------+---------------------+---------------------+---------+-----------------+--------------------+-----+--------------------+\n",
      "|anonymous|anonymous_to_peers|Participation|Opinion|Question|Answer|Sentiment|Confusion|Urgency|CourseType|course_display_name|created_at|post_type|up_count|comment_thread_id|reads|Opinion_tmp|  Opinion_Vec|Question_tmp| Question_Vec|Answer_tmp|   Answer_Vec|Sentiment_tmp| Sentiment_Vec|Confusion_tmp|  Confusion_Vec|Urgency_tmp|   Urgency_Vec|course_display_name_tmp|course_display_name_Vec|created_at_tmp|     created_at_Vec|post_type_tmp|post_type_Vec|up_count_tmp|  up_count_Vec|comment_thread_id_tmp|comment_thread_id_Vec|reads_tmp|        reads_Vec|            features|label|    selectedFeatures|\n",
      "+---------+------------------+-------------+-------+--------+------+---------+---------+-------+----------+-------------------+----------+---------+--------+-----------------+-----+-----------+-------------+------------+-------------+----------+-------------+-------------+--------------+-------------+---------------+-----------+--------------+-----------------------+-----------------------+--------------+-------------------+-------------+-------------+------------+--------------+---------------------+---------------------+---------+-----------------+--------------------+-----+--------------------+\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      7.0|      7.0|    1.0|       0.0|                0.0|    3272.0|      0.0|     0.0|           1178.0| 36.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          7.0|(12,[7],[1.0])|          7.0| (12,[7],[1.0])|        1.0|(12,[1],[1.0])|                    0.0|          (5,[0],[1.0])|        1338.0|(3754,[1338],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|               1142.0|  (1210,[1142],[1.0])|     36.0| (275,[36],[1.0])|(5313,[0,2,4,13,2...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "|    false|             false|         true|    1.0|     1.0|   0.0|      0.0|      4.0|    4.0|       0.0|                0.0|    3501.0|      0.0|     0.0|            469.0| 99.0|        1.0|(2,[1],[1.0])|         1.0|(2,[1],[1.0])|       0.0|(2,[0],[1.0])|          0.0|(12,[0],[1.0])|          4.0| (12,[4],[1.0])|        4.0|(12,[4],[1.0])|                    0.0|          (5,[0],[1.0])|        1800.0|(3754,[1800],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|                691.0|   (1210,[691],[1.0])|     97.0| (275,[97],[1.0])|(5313,[1,3,4,6,22...|  0.0|(10,[1,3,4,6],[1....|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      4.0|      2.0|    3.0|       0.0|                0.0|    1695.0|      0.0|     0.0|           1208.0| 48.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          4.0|(12,[4],[1.0])|          2.0| (12,[2],[1.0])|        3.0|(12,[3],[1.0])|                    0.0|          (5,[0],[1.0])|         330.0| (3754,[330],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|                489.0|   (1210,[489],[1.0])|     45.0| (275,[45],[1.0])|(5313,[0,2,4,10,2...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      5.0|      2.0|    3.0|       0.0|                0.0|    1167.0|      1.0|     0.0|              0.0| 11.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          5.0|(12,[5],[1.0])|          2.0| (12,[2],[1.0])|        3.0|(12,[3],[1.0])|                    0.0|          (5,[0],[1.0])|        2067.0|(3754,[2067],[1.0])|          1.0|(2,[1],[1.0])|         0.0|(25,[0],[1.0])|                  0.0|     (1210,[0],[1.0])|     11.0| (275,[11],[1.0])|(5313,[0,2,4,11,2...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      9.0|      7.0|    2.0|       0.0|                0.0|     778.0|      1.0|     0.0|              0.0|  2.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          9.0|(12,[9],[1.0])|          7.0| (12,[7],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|        1586.0|(3754,[1586],[1.0])|          1.0|(2,[1],[1.0])|         0.0|(25,[0],[1.0])|                  0.0|     (1210,[0],[1.0])|      2.0|  (275,[2],[1.0])|(5313,[0,2,4,15,2...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      6.0|      1.0|    2.0|       0.0|                0.0|    3557.0|      1.0|     1.0|              0.0|  2.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          6.0|(12,[6],[1.0])|          1.0| (12,[1],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|        2018.0|(3754,[2018],[1.0])|          1.0|(2,[1],[1.0])|         1.0|(25,[1],[1.0])|                  0.0|     (1210,[0],[1.0])|      2.0|  (275,[2],[1.0])|(5313,[0,2,4,12,1...|  0.0|(10,[0,2,4,9],[1....|\n",
      "|    false|             false|         true|    0.0|     0.0|   1.0|      0.0|     10.0|    2.0|       0.0|                0.0|    3370.0|      0.0|     0.0|             53.0|165.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       1.0|(2,[1],[1.0])|          0.0|(12,[0],[1.0])|         10.0|(12,[10],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|         189.0| (3754,[189],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|                 53.0|    (1210,[53],[1.0])|    165.0|(275,[165],[1.0])|(5313,[0,2,5,6,28...|  0.0|(10,[0,2,5,6],[1....|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      1.0|      2.0|    2.0|       0.0|                0.0|    3421.0|      1.0|     0.0|              0.0| 16.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          1.0|(12,[1],[1.0])|          2.0| (12,[2],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|        1412.0|(3754,[1412],[1.0])|          1.0|(2,[1],[1.0])|         0.0|(25,[0],[1.0])|                  0.0|     (1210,[0],[1.0])|     16.0| (275,[16],[1.0])|(5313,[0,2,4,7,20...|  0.0|(10,[0,2,4,7],[1....|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      7.0|      2.0|    2.0|       0.0|                0.0|     225.0|      0.0|     0.0|            654.0| 10.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          7.0|(12,[7],[1.0])|          2.0| (12,[2],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|        3359.0|(3754,[3359],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|                762.0|   (1210,[762],[1.0])|     10.0| (275,[10],[1.0])|(5313,[0,2,4,13,2...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "|    false|             false|         true|    0.0|     0.0|   0.0|      2.0|      7.0|    2.0|       0.0|                0.0|    1231.0|      0.0|     0.0|            452.0| 29.0|        0.0|(2,[0],[1.0])|         0.0|(2,[0],[1.0])|       0.0|(2,[0],[1.0])|          2.0|(12,[2],[1.0])|          7.0| (12,[7],[1.0])|        2.0|(12,[2],[1.0])|                    0.0|          (5,[0],[1.0])|        1090.0|(3754,[1090],[1.0])|          0.0|(2,[0],[1.0])|         0.0|(25,[0],[1.0])|                753.0|   (1210,[753],[1.0])|     29.0| (275,[29],[1.0])|(5313,[0,2,4,8,25...|  0.0|(10,[0,2,4],[1.0,...|\n",
      "+---------+------------------+-------------+-------+--------+------+---------+---------+-------+----------+-------------------+----------+---------+--------+-----------------+-----+-----------+-------------+------------+-------------+----------+-------------+-------------+--------------+-------------+---------------+-----------+--------------+-----------------------+-----------------------+--------------+-------------------+-------------+-------------+------------+--------------+---------------------+---------------------+---------+-----------------+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector as CSS\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('BDAS_Iteration4').setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "selector = CSS(numTopFeatures=10, featuresCol=\"features\",\n",
    "                         outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "selector_result = selector.fit(allData)\n",
    "selector_transformed = selector_result.transform(allData)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n",
    "selector_transformed.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Features Important\n",
    "from pyspark.ml.regression import DecisionTreeRegressor as DTR\n",
    "import matplotlib.pyplot as plt\n",
    "dtr_tree = DTR(labelCol = \"label\", featuresCol = \"features\", maxDepth = 3)\n",
    "dtr_tree_fit = dtr_tree.fit(trainingData)\n",
    "dtr_tree_transformed = dtr_tree_fit.transform(testData)\n",
    "\n",
    "dtr_tree_fit.featureImportances\n",
    "\n",
    "plt.figure(figsize=(23,6))\n",
    "plt.bar(\"label\", dtr_tree_fit.featureImportances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Decision Tree Feature Importance')\n",
    "plt.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Feature Importances\n",
    "from pyspark.ml.regression import RandomForestRegressor as RFR\n",
    "import matplotlib.pyplot as plt\n",
    "rfr_tree = RFR(labelCol = \"label\", featuresCol = \"features\", maxDepth = 3)\n",
    "rfr_tree_fit = rfr_tree.fit(trainingData)\n",
    "rfr_tree_transformed = rfr_tree_fit.transform(testData)\n",
    "\n",
    "rfr_tree_fit.featureImportances\n",
    "\n",
    "plt.figure(figsize=(23,6))\n",
    "plt.bar(\"label\", rfr_tree_fit.featureImportances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = [ 'Opinion',\n",
    "                'Question',\n",
    "                'Answer',\n",
    "                'Sentiment',\n",
    "                'Confusion',\n",
    "                'Urgency',\n",
    "                'course_display_name',\n",
    "                'created_at',\n",
    "                'post_type',\n",
    "                'up_count',\n",
    "                'comment_thread_id',\n",
    "                'reads',\n",
    "               'CourseType']\n",
    "\n",
    "print(dtc_model.toDebugString)\n",
    "\n",
    "index = 0 \n",
    "for feature in feature_name:\n",
    "    print(' feature ' + str(index) + \" : \" + feature_name[index])\n",
    "    index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
